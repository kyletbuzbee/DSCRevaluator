{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":13735896,"sourceType":"datasetVersion","datasetId":8739753},{"sourceId":13735964,"sourceType":"datasetVersion","datasetId":8739814},{"sourceId":13746270,"sourceType":"datasetVersion","datasetId":8746918},{"sourceId":646742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":487757,"modelId":503178}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸš€ **PROPERTY INSPECTOR AI - SINGLE FOLDER TRAINING**\n\n\n**Process one dataset folder at a time for incremental training!**\n\n\n\n## ðŸŽ¯ **What This Does:**\n\n- âœ… Single folder dataset processing\n\n- âœ… Automatic error handling \n\n- âœ… GPU-accelerated training\n\n- âœ… Export ready model for your app\n\n\n\n## ðŸ“Š **Expected Output:**\n\n- **Variable training images** (depends on folder selected)\n\n- **Dynamic classes** detected\n\n- **15-45 minutes** training time\n\n- **Ready model** for property evaluations\n\n\n\n---\n\n\n\n## ðŸ”§ **REQUIRED CONFIGURATION:**\n\n- **Set your folder path below** (see examples)\n- Add the entire BuildingInspection dataset as input\n\n---","metadata":{}},{"cell_type":"markdown","source":"## âš™ï¸ **FOLDER CONFIGURATION - EDIT THIS!**","metadata":{}},{"cell_type":"code","source":"# ðŸš¨ IMPORTANT: SET YOUR TARGET FOLDER PATH HERE ðŸš¨\n\n# Options - UNCOMMENT one:\n\n# For BuildingInspection chunks:\n# TARGET_FOLDER = \"/kaggle/input/building-inspection/BuildingInspection/chunks/chunk_001\"\n# TARGET_FOLDER = \"/kaggle/input/building-inspection/BuildingInspection/chunks/chunk_002\"\n# ... or any chunk from chunk_001 to chunk_048\n\n# For complete datasets:\n# TARGET_FOLDER = \"/kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb\"  # Home inspection\n# TARGET_FOLDER = \"/kaggle/input/building-inspection/BuildingInspection\"               # Full BuildingInspection (if available)\n# TARGET_FOLDER = \"/kaggle/input/building-inspection/buildingDefect\"                 # Building defects\n\n# EXAMPLE: Start with HomeInspection dataset\nTARGET_FOLDER = \"/kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb\"\n\nprint(f\"ðŸŽ¯ Target folder: {TARGET_FOLDER}\")\nprint(f\"ðŸ“ Dataset: {TARGET_FOLDER.split('/')[-1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:46:43.834098Z","iopub.execute_input":"2025-11-15T18:46:43.834774Z","iopub.status.idle":"2025-11-15T18:46:43.839514Z","shell.execute_reply.started":"2025-11-15T18:46:43.834749Z","shell.execute_reply":"2025-11-15T18:46:43.838640Z"}},"outputs":[{"name":"stdout","text":"ðŸŽ¯ Target folder: /kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb\nðŸ“ Dataset: HomeInspection.v1i.yolov8-obb\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"---\n# ðŸ“‹ COMPLETE SINGLE FOLDER TRAINING PIPELINE\n# Run this entire cell to train on your selected folder!","metadata":{}},{"cell_type":"code","source":"# This single cell contains everything needed for professional single-folder training\n\nprint(\"ðŸ—ï¸ SINGLE FOLDER PROPERTY INSPECTOR TRAINING\")\nprint(\"=\" * 60)\n\n# Define target folder at top of cell\ndataset_path = TARGET_FOLDER  # Uses the configuration from above\n\nprint(f\"ðŸ“ Training on: {dataset_path}\")\nprint(\"=\" * 60)\n\nprint(\"ðŸ”§ Phase 1: Environment & Dependencies Setup\")\nprint(\"=\" * 60)\n\n# Install dependencies\n!pip install ultralytics torch torchvision torchaudio --quiet\n!pip install roboflow opencv-python-headless matplotlib seaborn --quiet\n\n# Core imports\nimport torch\nfrom ultralytics import YOLO\nimport os\nimport shutil\nimport yaml\nfrom pathlib import Path\nimport glob\nfrom datetime import datetime\nimport sys\n\nprint(\"âœ… Dependencies installed\")\n\n# GPU check\nprint(f\"ðŸ–¥ï¸ GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"   GPU Model: {gpu_name}\")\n    print(f\"   VRAM: {gpu_memory:.1f} GB\")\nelse:\n    print(\"   âš ï¸ No GPU detected - training will be slow\")\n\n# Create directories\nos.makedirs('models', exist_ok=True)\nos.makedirs('dataset_processed', exist_ok=True)\nprint(\"ðŸ“ Directories created\")\n\n# Check if target dataset exists\nif not os.path.exists(dataset_path):\n    print(f\"âŒ ERROR: Dataset path not found: {dataset_path}\")\n    print(\"   Please check your TARGET_FOLDER configuration above\")\n    print(\"   Make sure you added the BuildingInspection dataset as Kaggle input\")\n    sys.exit(1)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ” Phase 2: Single Dataset Analysis & Loading\")\nprint(\"=\" * 60)\n\n# Analyze single dataset\ndef analyze_single_dataset(base_path):\n    info = {\n        'name': os.path.basename(base_path),\n        'path': base_path,\n        'exists': os.path.exists(base_path)\n    }\n    \n    if not info['exists']:\n        return info\n    \n    # Find data.yaml safely\n    data_yaml_paths = list(Path(base_path).rglob('data.yaml'))\n    if data_yaml_paths:\n        try:\n            with open(data_yaml_paths[0], 'r') as f:\n                data_config = yaml.safe_load(f)\n            info['data_yaml_path'] = str(data_yaml_paths[0])\n            info['config'] = data_config\n            info['data_yaml_dir'] = str(Path(data_yaml_paths[0]).parent)\n        except Exception as e:\n            print(f\"   Warning: Could not read data.yaml: {e}\")\n    \n    # Count images and labels safely\n    total_images = 0\n    total_labels = 0\n    \n    for split in ['train', 'valid', 'test', 'images', 'labels']:\n        split_dir = Path(base_path) / split\n        if split_dir.exists():\n            images_in_split = len(list(split_dir.rglob('*.jpg'))) + len(list(split_dir.rglob('*.png'))) + len(list(split_dir.rglob('*.jpeg')))\n            labels_in_split = len(list(split_dir.rglob('*.txt')))\n            \n            if images_in_split > 0:\n                info[f'{split}_images'] = images_in_split\n                total_images += images_in_split\n            if labels_in_split > 0:\n                info[f'{split}_labels'] = labels_in_split\n                total_labels += labels_in_split\n    \n    # Try nested structures\n    for img_dir in ['images', 'train', 'valid', 'test']:\n        full_img_path = Path(base_path) / img_dir\n        if full_img_path.exists():\n            for sub_dir in ['train', 'val', 'test']:\n                sub_img_path = full_img_path / sub_dir\n                if sub_img_path.exists():\n                    sub_images = len(list(sub_img_path.rglob('*.jpg'))) + len(list(sub_img_path.rglob('*.png'))) + len(list(sub_img_path.rglob('*.jpeg')))\n                    if sub_images > 0:\n                        info[f'{img_dir}_{sub_dir}_images'] = sub_images\n                        total_images += sub_images\n                        \n                        # Look for corresponding labels\n                        label_path = Path(base_path) / 'labels' / sub_dir\n                        if not label_path.exists():\n                            label_path = full_img_path.parent / 'labels' / sub_dir\n                        if label_path.exists():\n                            sub_labels = len(list(label_path.rglob('*.txt')))\n                            info[f'{img_dir}_{sub_dir}_labels'] = sub_labels\n                            total_labels += sub_labels\n    \n    info['total_images'] = total_images\n    info['total_labels'] = total_labels\n    \n    return info\n\n# Analyze the target dataset\ndataset_info = analyze_single_dataset(dataset_path)\n\nif not dataset_info['exists']:\n    print(f\"âŒ ERROR: Dataset folder not found: {dataset_path}\")\n    print(\"   Please check your TARGET_FOLDER configuration\")\n    sys.exit(1)\n\nprint(f\"ðŸ“‹ Dataset Analysis: {dataset_info['name']}\")\nprint(f\"   ðŸ“ Path: {dataset_info['path']}\")\nprint(f\"   ðŸ“Š Total Images: {dataset_info['total_images']}\")\nprint(f\"   ðŸ·ï¸ Total Labels: {dataset_info['total_labels']}\")\n\n# Show detailed info\nif 'config' in dataset_info:\n    config = dataset_info['config']\n    if 'names' in config:\n        num_classes = len(config['names'])\n        print(f\"   ðŸŽ¯ Classes ({num_classes}): {list(config['names'].values())}\")\n    else:\n        print(f\"   âš ï¸ No class names found in data.yaml\")\n\n# Show split details\nfor key in sorted(dataset_info.keys()):\n    if key.endswith(('_images', '_labels')) and key != 'total_images' and key != 'total_labels':\n        print(f\"   {key}: {dataset_info[key]}\")\n\n# Validate we have training data\nif dataset_info['total_images'] == 0:\n    print(f\"\\nâŒ ERROR: No images found in dataset folder!\")\n    print(\"   Check your folder structure and file formats (.jpg, .png)\")\n    sys.exit(1)\n\nprint(f\"\\nâœ… Dataset ready for training!\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ¤– Phase 3: AI Model Setup & Training\")\nprint(\"=\" * 60)\n\n# Prepare data configuration\ndata_yaml_path = None\ntraining_data_path = dataset_path\n\nif 'data_yaml_path' in dataset_info:\n    data_yaml_path = dataset_info['data_yaml_path']\n    print(f\"ðŸ“„ Using existing data.yaml: {data_yaml_path}\")\nelse:\n    # Create a custom data.yaml for this folder\n    print(\"ðŸ”§ Creating data configuration...\")\n    \n    # Try to auto-detect paths\n    train_path = \"\"\n    val_path = \"\"\n    test_path = \"\"\n    \n    # Check different possible structures\n    possible_paths = [\n        (\"train\", Path(dataset_path) / \"train\"),\n        (\"train\", Path(dataset_path) / \"images\" / \"train\"),\n        (\"valid\", Path(dataset_path) / \"valid\"),\n        (\"valid\", Path(dataset_path) / \"images\" / \"val\"),\n        (\"test\", Path(dataset_path) / \"test\"),\n        (\"test\", Path(dataset_path) / \"images\" / \"test\")\n    ]\n    \n    class_names = []\n    detected_classes = set()\n    \n    for split_type, split_path in possible_paths:\n        if split_path.exists():\n            if split_type == \"train\":\n                train_path = split_path.relative_to(Path(dataset_path))\n            elif split_type == \"valid\":\n                val_path = split_path.relative_to(Path(dataset_path))\n            elif split_type == \"test\":\n                test_path = split_path.relative_to(Path(dataset_path))\n                \n            # Try to detect classes from labels\n            label_path = split_path.parent / \"labels\" / split_path.name if (split_path.parent / \"labels\" / split_path.name).exists() else split_path\n            if label_path.exists():\n                for txt_file in label_path.glob(\"*.txt\"):\n                    try:\n                        with open(txt_file, 'r') as f:\n                            for line in f:\n                                parts = line.strip().split()\n                                if len(parts) >= 5:\n                                    detected_classes.add(int(parts[0]))\n                    except:\n                        continue\n    \n    # Create class names\n    max_class = max(detected_classes) if detected_classes else 0\n    class_names = [f\"class_{i}\" for i in range(max_class + 1)]\n    \n    # Create data.yaml\n    custom_config = {\n        'path': f'/kaggle/working/dataset_processed',\n        'names': {i: name for i, name in enumerate(class_names)},\n        'nc': len(class_names)\n    }\n    \n    if train_path:\n        custom_config['train'] = str(train_path)\n    if val_path:\n        custom_config['val'] = str(val_path)\n    if test_path:\n        custom_config['test'] = str(test_path)\n    \n    data_yaml_path = '/kaggle/working/dataset_processed/data.yaml'\n    \n    # Copy dataset to working directory for processing\n    processed_path = Path('/kaggle/working/dataset_processed')\n    if processed_path.exists():\n        shutil.rmtree(processed_path)\n    shutil.copytree(dataset_path, processed_path)\n    \n    # Write data.yaml\n    with open(data_yaml_path, 'w') as f:\n        yaml.safe_dump(custom_config, f, default_flow_style=False)\n    \n    training_data_path = '/kaggle/working/dataset_processed'\n    \n    print(f\"   Created data.yaml with {len(class_names)} classes\")\n    print(f\"   Training data copied to: {training_data_path}\")\n\n# Model setup\nprint(\"\\nðŸš€ Loading YOLOv8 model...\")\nmodel = YOLO('yolov8m.pt')\nprint(f\"âœ… YOLOv8m loaded\")\n\n# Training configuration\nepochs = 50 if dataset_info['total_images'] < 1000 else 30  # Adjust based on dataset size\n\ntraining_config = {\n    'data': data_yaml_path,\n    'epochs': epochs,\n    'batch': 8,  # Smaller batch for single datasets\n    'imgsz': 640,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'patience': 20,\n    'save': True,\n    'save_period': 25,\n    'project': 'runs/single_folder',\n    'name': f\"{dataset_info['name']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n    'exist_ok': True,\n    'pretrained': True,\n    'freeze': 10,\n    'lr0': 0.0001,\n    'lrf': 0.001,\n    'momentum': 0.937,\n    'weight_decay': 0.0005,\n    'warmup_epochs': 3.0,\n    'warmup_momentum': 0.8,\n    'warmup_bias_lr': 0.1\n}\n\nprint(f\"âš™ï¸ Training configuration:\")\nprint(f\"   Epochs: {training_config['epochs']}\")\nprint(f\"   Batch size: {training_config['batch']}\")\nprint(f\"   Device: {training_config['device']}\")\nprint(f\"   Images: {dataset_info['total_images']}\")\n\n# Start training\nprint(\"\\nðŸš€ STARTING TRAINING NOW!\")\nprint(f\"ðŸ’¡ Training {dataset_info['name']} for {training_config['epochs']} epochs\")\nprint(\"ðŸ’¡ Progress will appear below\")\nprint(\"=\" * 80)\n\nstart_time = datetime.now()\n\n# The main training call\nresults = model.train(**training_config)\n\n# Training complete\ntraining_time = datetime.now() - start_time\nhours = training_time.total_seconds() / 3600\n\nprint(\"=\" * 80)\nprint(\"ðŸŽ‰ TRAINING COMPLETE!\")\nprint(\"=\" * 80)\nprint(f\"â±ï¸ Training duration: {hours:.2f} hours\")\nprint(f\"ðŸ“ Dataset: {dataset_info['name']}\")\nprint(f\"ðŸ“Š Trained on {dataset_info['total_images']} images\")\nprint(f\"ðŸŽ¯ Model saved to: runs/single_folder/\")\n\n# Display metrics if available\nif hasattr(results, 'results_dict') and results.results_dict:\n    metrics = results.results_dict\n    print(\"\\nðŸ“Š Final Performance Metrics:\")\n    for key, value in metrics.items():\n        if key.startswith('metrics/'):\n            metric_name = key.replace('metrics/', '')\n            if isinstance(value, (int, float)):\n                print(f\"   {metric_name}: {value:.4f}\")\n            else:\n                print(f\"   {metric_name}: {value}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ“¦ Phase 4: Model Export & Download Prep\")\nprint(\"=\" * 60)\n\n# Find the trained model\nrun_dir = Path('runs/single_folder')\nbest_model_path = None\nlast_model_path = None\n\nif run_dir.exists():\n    for weights_dir in run_dir.rglob('weights'):\n        best_path = weights_dir / 'best.pt'\n        last_path = weights_dir / 'last.pt'\n        if best_path.exists():\n            best_model_path = best_path\n        if last_path.exists():\n            last_model_path = last_path\n\nif best_model_path:\n    try:\n        print(\"ðŸ§ª Found trained model - preparing for export...\")\n        \n        # Create export package\n        package_dir = f\"model_export_{dataset_info['name']}\"\n        if os.path.exists(package_dir):\n            shutil.rmtree(package_dir)\n        os.makedirs(package_dir, exist_ok=True)\n        \n        # Copy best model\n        weights_dir = Path(package_dir)\n        shutil.copy2(best_model_path, weights_dir / 'best.pt')\n        print(\"ðŸ“‹ Included: best.pt\")\n        \n        # Copy last model if available\n        if last_model_path:\n            shutil.copy2(last_model_path, weights_dir / 'last.pt')\n            print(\"ðŸ“‹ Included: last.pt\")\n        \n        # Copy data configuration\n        if os.path.exists(data_yaml_path):\n            shutil.copy2(data_yaml_path, Path(package_dir) / 'data.yaml')\n            print(\"ðŸ“‹ Included: data.yaml\")\n        \n        # Create integration instructions (fixed version)\n        instructions = f\"\"\"# Single Folder Trained Model: {dataset_info['name']}\n# Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## âœ… Training Complete\n- Dataset: {dataset_info['name']}\n- Folder: {TARGET_FOLDER}\n- Images: {dataset_info['total_images']}\n- Labels: {dataset_info['total_labels']}\n- Classes: {len(class_names) if 'class_names' in locals() else 'N/A'}\n- Training time: {hours:.2f} hours\n\n## ðŸ”§ Integration Instructions\n\n1. Extract this zip\n2. Copy weights/best.pt to your ml-service/runs/train/\n3. Update your model loading path\n4. Deploy and test with property images\n\n## ðŸ“ˆ Next Steps\n- Train next chunk/folder\n- Consider combining models for better performance\n- Test inference on your property images\n\"\"\"\n        \n        with open(f'{package_dir}/README.md', 'w') as f:\n            f.write(instructions)\n        print(\"ðŸ“‹ Created: README.md\")\n        \n        # Create zip file\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n        zip_filename = f'{dataset_info[\"name\"]}_{timestamp}.zip'\n        \n        if os.path.exists(zip_filename):\n            os.remove(zip_filename)\n        \n        shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', package_dir)\n        \n        # Get file size\n        file_size = os.path.getsize(zip_filename) / (1024 * 1024)\n        \n        print(f\"\\nðŸŽ‰ Model package created successfully!\")\n        print(f\"ðŸ“¦ File: {zip_filename}\")\n        print(f\"ðŸ“Š Size: {file_size:.1f} MB\")\n        print(\"\\nðŸ“¥ DOWNLOAD INSTRUCTIONS:\")\n        print(\"1. Go to Output tab (right panel)\")\n        print(f\"2. Download {zip_filename}\")\n        print(\"3. Extract to your DSCREvaluator ml-service/ directory\")\n        print(\"4. Update model path in your inference code\")\n        print(\"\\nðŸ† MISSION ACCOMPLISHED FOR THIS FOLDER! ðŸ†\")\n        \n    except Exception as e:\n        print(f\"âŒ Export error: {e}\")\n\nelse:\n    print(\"âŒ No trained model found for export\")\n    print(\"   Training may have failed or not completed\")\n    print(\"   Check the training output above for errors\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ðŸŽŠ SINGLE FOLDER TRAINING COMPLETE - PROPERTY INSPECTOR READY!\")\nprint(\"=\" * 80)\nprint(f\"\\nðŸ¤– Successfully trained on: {dataset_info['name']}\")\nprint(f\"ðŸ“Š Dataset size: {dataset_info['total_images']} images\")\nprint(f\"ðŸ’¼ Ready for integration into your DSCREvaluator application\")\nprint(f\"\\n\\nðŸŽ¯ Change TARGET_FOLDER at the top and run again for next folder!\")\nprint(\"\\nðŸ”” Remember to download the model package from Output tab!\")\nprint(\"\\nðŸ‡ºðŸ‡¸ Made in America ðŸ‡ºðŸ‡¸\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T18:46:46.421885Z","iopub.execute_input":"2025-11-15T18:46:46.422450Z","iopub.status.idle":"2025-11-15T18:47:05.570506Z","shell.execute_reply.started":"2025-11-15T18:46:46.422425Z","shell.execute_reply":"2025-11-15T18:47:05.568981Z"}},"outputs":[{"name":"stdout","text":"ðŸ—ï¸ SINGLE FOLDER PROPERTY INSPECTOR TRAINING\n============================================================\nðŸ“ Training on: /kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb\n============================================================\nðŸ”§ Phase 1: Environment & Dependencies Setup\n============================================================\nâœ… Dependencies installed\nðŸ–¥ï¸ GPU Available: True\n   GPU Model: Tesla P100-PCIE-16GB\n   VRAM: 15.9 GB\nðŸ“ Directories created\n\n============================================================\nðŸ” Phase 2: Single Dataset Analysis & Loading\n============================================================\nðŸ“‹ Dataset Analysis: HomeInspection.v1i.yolov8-obb\n   ðŸ“ Path: /kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb\n   ðŸ“Š Total Images: 444\n   ðŸ·ï¸ Total Labels: 444\n   ðŸŽ¯ Classes (16): ['Corrosion', 'abrasion', 'alligatoring', 'black mold', 'blistering', 'buckling', 'chalking', 'crack', 'damp', 'efflorescence', 'leaks', 'mildew', 'peeling', 'shagging', 'spalling', 'stainning building']\n   train_images: 443\n   train_labels: 443\n   valid_images: 1\n   valid_labels: 1\n\nâœ… Dataset ready for training!\n\n============================================================\nðŸ¤– Phase 3: AI Model Setup & Training\n============================================================\nðŸ“„ Using existing data.yaml: /kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb/data.yaml\n\nðŸš€ Loading YOLOv8 model...\nâœ… YOLOv8m loaded\nâš™ï¸ Training configuration:\n   Epochs: 50\n   Batch size: 8\n   Device: cuda\n   Images: 444\n\nðŸš€ STARTING TRAINING NOW!\nðŸ’¡ Training HomeInspection.v1i.yolov8-obb for 50 epochs\nðŸ’¡ Progress will appear below\n================================================================================\nUltralytics 8.3.228 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/building-inspection/HomeInspection.v1i.yolov8-obb/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.001, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=HomeInspection.v1i.yolov8-obb_20251115_1846, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/single_folder, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/single_folder/HomeInspection.v1i.yolov8-obb_20251115_1846, save_frames=False, save_json=False, save_period=25, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 15.9MB/s 0.0s\nOverriding model.yaml nc=80 with nc=16\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n 22        [15, 18, 21]  1   3784960  ultralytics.nn.modules.head.Detect           [16, [192, 384, 576]]         \nModel summary: 169 layers, 25,865,584 parameters, 25,865,568 gradients, 79.1 GFLOPs\n\nTransferred 469/475 items from pretrained weights\nFreezing layer 'model.0.conv.weight'\nFreezing layer 'model.0.bn.weight'\nFreezing layer 'model.0.bn.bias'\nFreezing layer 'model.1.conv.weight'\nFreezing layer 'model.1.bn.weight'\nFreezing layer 'model.1.bn.bias'\nFreezing layer 'model.2.cv1.conv.weight'\nFreezing layer 'model.2.cv1.bn.weight'\nFreezing layer 'model.2.cv1.bn.bias'\nFreezing layer 'model.2.cv2.conv.weight'\nFreezing layer 'model.2.cv2.bn.weight'\nFreezing layer 'model.2.cv2.bn.bias'\nFreezing layer 'model.2.m.0.cv1.conv.weight'\nFreezing layer 'model.2.m.0.cv1.bn.weight'\nFreezing layer 'model.2.m.0.cv1.bn.bias'\nFreezing layer 'model.2.m.0.cv2.conv.weight'\nFreezing layer 'model.2.m.0.cv2.bn.weight'\nFreezing layer 'model.2.m.0.cv2.bn.bias'\nFreezing layer 'model.2.m.1.cv1.conv.weight'\nFreezing layer 'model.2.m.1.cv1.bn.weight'\nFreezing layer 'model.2.m.1.cv1.bn.bias'\nFreezing layer 'model.2.m.1.cv2.conv.weight'\nFreezing layer 'model.2.m.1.cv2.bn.weight'\nFreezing layer 'model.2.m.1.cv2.bn.bias'\nFreezing layer 'model.3.conv.weight'\nFreezing layer 'model.3.bn.weight'\nFreezing layer 'model.3.bn.bias'\nFreezing layer 'model.4.cv1.conv.weight'\nFreezing layer 'model.4.cv1.bn.weight'\nFreezing layer 'model.4.cv1.bn.bias'\nFreezing layer 'model.4.cv2.conv.weight'\nFreezing layer 'model.4.cv2.bn.weight'\nFreezing layer 'model.4.cv2.bn.bias'\nFreezing layer 'model.4.m.0.cv1.conv.weight'\nFreezing layer 'model.4.m.0.cv1.bn.weight'\nFreezing layer 'model.4.m.0.cv1.bn.bias'\nFreezing layer 'model.4.m.0.cv2.conv.weight'\nFreezing layer 'model.4.m.0.cv2.bn.weight'\nFreezing layer 'model.4.m.0.cv2.bn.bias'\nFreezing layer 'model.4.m.1.cv1.conv.weight'\nFreezing layer 'model.4.m.1.cv1.bn.weight'\nFreezing layer 'model.4.m.1.cv1.bn.bias'\nFreezing layer 'model.4.m.1.cv2.conv.weight'\nFreezing layer 'model.4.m.1.cv2.bn.weight'\nFreezing layer 'model.4.m.1.cv2.bn.bias'\nFreezing layer 'model.4.m.2.cv1.conv.weight'\nFreezing layer 'model.4.m.2.cv1.bn.weight'\nFreezing layer 'model.4.m.2.cv1.bn.bias'\nFreezing layer 'model.4.m.2.cv2.conv.weight'\nFreezing layer 'model.4.m.2.cv2.bn.weight'\nFreezing layer 'model.4.m.2.cv2.bn.bias'\nFreezing layer 'model.4.m.3.cv1.conv.weight'\nFreezing layer 'model.4.m.3.cv1.bn.weight'\nFreezing layer 'model.4.m.3.cv1.bn.bias'\nFreezing layer 'model.4.m.3.cv2.conv.weight'\nFreezing layer 'model.4.m.3.cv2.bn.weight'\nFreezing layer 'model.4.m.3.cv2.bn.bias'\nFreezing layer 'model.5.conv.weight'\nFreezing layer 'model.5.bn.weight'\nFreezing layer 'model.5.bn.bias'\nFreezing layer 'model.6.cv1.conv.weight'\nFreezing layer 'model.6.cv1.bn.weight'\nFreezing layer 'model.6.cv1.bn.bias'\nFreezing layer 'model.6.cv2.conv.weight'\nFreezing layer 'model.6.cv2.bn.weight'\nFreezing layer 'model.6.cv2.bn.bias'\nFreezing layer 'model.6.m.0.cv1.conv.weight'\nFreezing layer 'model.6.m.0.cv1.bn.weight'\nFreezing layer 'model.6.m.0.cv1.bn.bias'\nFreezing layer 'model.6.m.0.cv2.conv.weight'\nFreezing layer 'model.6.m.0.cv2.bn.weight'\nFreezing layer 'model.6.m.0.cv2.bn.bias'\nFreezing layer 'model.6.m.1.cv1.conv.weight'\nFreezing layer 'model.6.m.1.cv1.bn.weight'\nFreezing layer 'model.6.m.1.cv1.bn.bias'\nFreezing layer 'model.6.m.1.cv2.conv.weight'\nFreezing layer 'model.6.m.1.cv2.bn.weight'\nFreezing layer 'model.6.m.1.cv2.bn.bias'\nFreezing layer 'model.6.m.2.cv1.conv.weight'\nFreezing layer 'model.6.m.2.cv1.bn.weight'\nFreezing layer 'model.6.m.2.cv1.bn.bias'\nFreezing layer 'model.6.m.2.cv2.conv.weight'\nFreezing layer 'model.6.m.2.cv2.bn.weight'\nFreezing layer 'model.6.m.2.cv2.bn.bias'\nFreezing layer 'model.6.m.3.cv1.conv.weight'\nFreezing layer 'model.6.m.3.cv1.bn.weight'\nFreezing layer 'model.6.m.3.cv1.bn.bias'\nFreezing layer 'model.6.m.3.cv2.conv.weight'\nFreezing layer 'model.6.m.3.cv2.bn.weight'\nFreezing layer 'model.6.m.3.cv2.bn.bias'\nFreezing layer 'model.7.conv.weight'\nFreezing layer 'model.7.bn.weight'\nFreezing layer 'model.7.bn.bias'\nFreezing layer 'model.8.cv1.conv.weight'\nFreezing layer 'model.8.cv1.bn.weight'\nFreezing layer 'model.8.cv1.bn.bias'\nFreezing layer 'model.8.cv2.conv.weight'\nFreezing layer 'model.8.cv2.bn.weight'\nFreezing layer 'model.8.cv2.bn.bias'\nFreezing layer 'model.8.m.0.cv1.conv.weight'\nFreezing layer 'model.8.m.0.cv1.bn.weight'\nFreezing layer 'model.8.m.0.cv1.bn.bias'\nFreezing layer 'model.8.m.0.cv2.conv.weight'\nFreezing layer 'model.8.m.0.cv2.bn.weight'\nFreezing layer 'model.8.m.0.cv2.bn.bias'\nFreezing layer 'model.8.m.1.cv1.conv.weight'\nFreezing layer 'model.8.m.1.cv1.bn.weight'\nFreezing layer 'model.8.m.1.cv1.bn.bias'\nFreezing layer 'model.8.m.1.cv2.conv.weight'\nFreezing layer 'model.8.m.1.cv2.bn.weight'\nFreezing layer 'model.8.m.1.cv2.bn.bias'\nFreezing layer 'model.9.cv1.conv.weight'\nFreezing layer 'model.9.cv1.bn.weight'\nFreezing layer 'model.9.cv1.bn.bias'\nFreezing layer 'model.9.cv2.conv.weight'\nFreezing layer 'model.9.cv2.bn.weight'\nFreezing layer 'model.9.cv2.bn.bias'\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 66.7MB/s 0.1s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2816659672.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;31m# The main training call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;31m# Training complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_setup_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single-GPU and DDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mcallbacks_backup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backup callbacks as check_amp() resets them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_amp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_backup\u001b[0m  \u001b[0;31m# restore callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# DDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\u001b[0m in \u001b[0;36mcheck_amp\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mamp_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo11n.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix}checks passed âœ…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\u001b[0m in \u001b[0;36mamp_allclose\u001b[0;34m(m, im)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mimgsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# max stride P5-32 and P6-64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# FP32 inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# AMP inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;34m...\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Detected {len(r)} objects in image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     def track(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# Preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim0s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnot_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# BGR to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mpre_transform\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mletterbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mletterbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/augment.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, labels, image)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnew_unpad\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_unpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"],"ename":"error","evalue":"OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n","output_type":"error"}],"execution_count":17}]}